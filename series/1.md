# Beginner-Friendly Guide to System Design

This guide covers the fundamentals of building a scalable, robust system design, inspired by real-world examples like Amazon's e-commerce platform. It explains key components (e.g., load balancers, API gateways, caching, queues) and how they interact in a cloud environment. The focus is on high-level concepts like microservices, scaling strategies, and fault tolerance.

## Core System Structure: Client and Server

- **Client**: Any device or user interacting with the system.
  - Examples: Mobile devices, laptops, IoT devices.
  - Role: Sends requests to the server.

- **Server**: A machine running 24/7 to handle requests.
  - Characteristics: Has a public IP address for global access.
  - Can be on-premises (e.g., your laptop, if kept running) or cloud-based (e.g., AWS virtual machines).
  - Cloud providers (e.g., AWS, DigitalOcean) provide reliable 24/7 machines with public IPs.

- **IP Address Analogy**: Like a home address (street, city, pin code, state, country) to locate the server on the internet.
  - Public IP: Globally accessible but hard to remember (e.g., 10.2.3.4).

- **DNS (Domain Name System)**:
  - Acts as a global directory mapping domains (e.g., amazon.com) to IP addresses.
  - Process: **DNS Resolution**—User types amazon.com; DNS returns the IP; request routes to the server.
  - Decentralized: Paid service when buying a domain; maintains domain-IP pairs like a phone directory.

## Scaling Strategies

### Vertical Scaling (Scaling Up)
- **Definition**: Increase resources on a single server (e.g., add CPU, RAM, disk space).
- **Example**:
  - Start: 2 CPUs, 4GB RAM.
  - Upgrade: 64 CPUs, 128GB RAM (handles more traffic).
  - Extreme: 6000 CPUs, 128000GB RAM (supercomputer for peak loads).
- **Pros**: Handles more load on one machine.
- **Cons**:
  - Over-optimization: Wastes resources during low traffic (e.g., Amazon's traffic peaks during festivals, dips in recessions).
  - Downtime: Requires server restart to add resources—unsuitable for sudden spikes (e.g., Amazon's Big Billion Days sale at midnight).
  - Not ideal for zero-downtime needs.
- **Cloud Optimization**: Use auto-scaling policies—start minimal (2 CPUs, 4GB), auto-increase during traffic spikes; pay only for used resources.
  - Analogy: Restaurant with base cooks (always paid) + hourly hires for rush (pay per hour).

### Horizontal Scaling (Scaling Out/In)
- **Definition**: Add/remove multiple server replicas instead of upgrading one.
- **Example**:
  - Start: One server (IP: 10.2.3.5).
  - Scale Out: Spin up parallel servers (IPs: 10.2.3.6, 10.2.3.7) during load increase.
  - Scale In: Remove servers when load decreases.
- **Pros**:
  - Zero downtime: New servers boot in parallel (e.g., existing servers handle traffic while new ones start).
  - Flexible: Site slows slightly but doesn't crash.
- **Cons**: Multiple IPs—users/DNS can't route to all; needs load balancer.
- **Management**: Auto-scaling policies monitor traffic and adjust replicas dynamically.

## Load Balancer

- **Role**: Distributes incoming traffic across multiple servers to prevent overload.
- **Setup**: Register load balancer's IP (e.g., 10.2.3.7) in DNS; all requests route here first.
- **Algorithms**:
  - **Round Robin**: Cycles requests equally (1st to Server 1, 2nd to Server 2, etc.).
- **Health Checks**: New servers register with load balancer; it verifies they're healthy (up for ~1 minute) before routing traffic.
- **Elastic Load Balancer (ELB)**: AWS term—handles high loads with internal redundancy (multiple threads/workers).
- **In Vertical Scaling**: Not needed (single server).
- **In Horizontal Scaling**: Essential for routing to replicas.

## Microservices Architecture

- **Overview**: Break app into independent services (e.g., auth, orders, payments, API).
  - Each service scales independently (e.g., API: 6 servers; Payments: 2 servers; Auth: 4 servers; Orders: 3 servers).
- **Routing Needs**: Direct requests by path (e.g., /auth → Auth service; /orders → Orders service).

### API Gateway
- **Role**: Centralized entry point; routes requests to appropriate services based on rules (host/path).
  - Acts as reverse proxy: Forwards client requests to backends and returns responses.
  - Example Rules:
    - /auth → Auth service load balancer.
    - /orders → Orders service load balancer.
    - /payments → Payments service load balancer.
    - / → API service load balancer.
- **Setup**: Own public IP registered in DNS; routes to service-specific load balancers.
- **Integration**: Can attach authentication (e.g., Auth0).
- **AWS Example**: Amazon API Gateway—routes to Lambda, EC2 (via load balancer), S3, or external services.
- **EC2 (Elastic Compute)**: AWS VMs hosting service replicas.

## Batch Processing and Asynchronous Communication

### Background Workers
- **Use Cases**: Bulk tasks (e.g., email 1M users for sale; process bulk Excel uploads).
- **Example**: Email Worker—reads from DB, sends emails via external API (e.g., Gmail).

### Synchronous vs. Asynchronous
- **Synchronous**: Payment service calls Email Worker directly (HTTP POST); waits for response.
  - Problem: Blocks during high load (e.g., emails/second); not scalable.
- **Asynchronous**: Use queues for non-blocking communication.

### Queue Systems (SQS - Simple Queue Service)
- **Role**: FIFO (First In, First Out) for async tasks; decouples services.
- **Example**:
  - Payment service pushes order details (e.g., order ID, email) to queue on payment.
  - Email Worker pulls events, sends email, discards (email arrives 2-3 seconds later).
- **Scaling**: Add more workers horizontally for parallelism (process multiple emails simultaneously).
- **Mechanisms**:
  - **Pull (Polling)**: Worker asks queue for messages (e.g., every 2 seconds).
    - **Short Polling**: Frequent checks—increases API calls/costs.
    - **Long Polling**: Wait 10 seconds, pull all events at once—more efficient.
  - **Push**: Queue invokes worker (not default in SQS).
- **Rate Limiting Integration**: Limit pulls (e.g., ≤10 emails/second) to respect external API limits (e.g., Gmail's 10 emails/second).
- **Other Queues**: CSV Queue for bulk uploads—worker processes and inserts into DB (e.g., MongoDB).

### Pub/Sub Model (SNS - Simple Notification Service)
- **Role**: Event-driven; one publisher broadcasts to multiple subscribers (one-to-many).
- **Example**:
  - Payment service publishes "payment made" event.
  - Subscribers: Email Worker, WhatsApp service, SMS service—all listen and act.
- **vs. Queue (SQS)**: SQS is one-to-one (guaranteed once); SNS is one-to-many (no uniqueness guarantee).
- **Pros**: Efficient for multi-action events (e.g., email + WhatsApp + SMS on payment).
- **Cons**: No built-in acknowledgement—events may be lost if no listener or processing fails.

### Acknowledgement and Error Handling
- **In Queues (SQS)**: Worker acknowledges success; failures → re-enqueue or Dead Letter Queue (DLQ) for retry (e.g., after 5-10 minutes if Gmail down).
- **In Pub/Sub (SNS)**: No automatic ACK—build custom retry/retain logic.

### Fan-Out Architecture
- **Role**: Combines SNS (publish) + multiple SQS queues for reliable multi-service notification with ACKs.
- **Example**:
  - Payment publishes to SNS.
  - SNS fans out to queues: WhatsApp Queue, Email Queue, SMS Queue.
  - Each queue has dedicated workers (EC2 instances) for processing.
- **Pros**:
  - Notifies multiple services.
  - Handles ACKs/retries per queue (e.g., retry failed SMS).
- **Real-World**: Video upload to S3 → SNS notifies → queues for transcription (480p, 360p, audio), thumbnail generation.
- **Pattern**: API Gateway → Lambda → SNS → Multiple SQS + Consumers.

## Rate Limiting

- **Definition**: Limit requests per time unit to prevent overload/DDoS.
- **Implementation**: At API Gateway/Load Balancer (e.g., 5 requests/second; block excess with "Too Many Requests" error).
- **Strategies**:
  - **Token Bucket**: Bucket fills with tokens at fixed rate; each request consumes one.
  - **Leaky Bucket**: Requests queue and "leak" out at constant rate.
- **Other Uses**: Control outbound rates (e.g., limit emails to external APIs).
- **Resources**: Read articles on strategies with code/diagrams (e.g., customer helpline, YouTube video processing).

## Database Scaling and Optimization

- **Challenges**: Single DB overwhelms with reads/writes from growing services.
- **Read Replicas**:
  - Master/Primary Node: Handles all writes; real-time reads.
  - Replicas: For analytics/logs (slight delay OK); offloads reads from primary.
- **Caching (e.g., Redis - In-Memory DB)**:
  - Check cache first; miss → query DB, cache result, return.
  - Benefit: Reduces DB calls; faster responses.

## Content Delivery Network (CDN)

- **Role**: Distributes static content (e.g., images/videos) globally to reduce latency/load.
- **AWS**: CloudFront—deploys edge servers worldwide.
- **Anycast IP**: Single IP routes users to nearest edge (e.g., North India users to local edge).
- **Process**:
  - User requests photo → Nearest edge checks cache.
  - Miss: Fetches from origin (load balancer/server), caches, serves.
  - Hit: Serves from cache (fast, low latency).
- **Benefits**:
  - Saves latency (e.g., Indian user avoids US server).
  - Reduces server load/bandwidth (e.g., Amazon product images cached regionally).
  - Improves user experience (faster loads).

## Advanced Topics (Teased for Future)
- Containerization (Docker).
- Orchestration (multi-container load balancing).
- Building highly available systems.

## Conclusion
This covers basic components for a scalable system: Client → DNS → CDN → API Gateway → Load Balancer → Microservices (EC2) + Queues/Pub-Sub + DB/Caching. Together, they ensure fault tolerance, zero downtime, and efficiency. For interviews or building apps, master these for robust designs.